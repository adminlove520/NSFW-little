import os
import sqlite3
import yaml
import json
import asyncio
import requests
import random
from datetime import datetime
from playwright.async_api import async_playwright

__version__ = "0.1.2"

# åŠ è½½é…ç½®
def load_config():
    with open("config.yaml", "r", encoding="utf-8") as f:
        return yaml.safe_load(f)

# åˆå§‹åŒ–æ•°æ®åº“
def init_db(db_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            site_name TEXT,
            url TEXT UNIQUE,
            title TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    conn.commit()
    conn.close()

# æ£€æŸ¥é“¾æ¥æ˜¯å¦å·²æ¨é€
def is_new_link(db_path, url):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT id FROM history WHERE url = ?", (url,))
    result = cursor.fetchone()
    conn.close()
    return result is None

# ä¿å­˜æ¨é€è®°å½•
def save_link(db_path, site_name, url, title):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    try:
        cursor.execute("INSERT INTO history (site_name, url, title) VALUES (?, ?, ?)", 
                       (site_name, url, title))
        conn.commit()
    except sqlite3.IntegrityError:
        pass
    conn.close()

# å‘é€ Discord Webhook
def send_discord_webhook(config, item):
    webhook_url = os.environ.get(config['discord']['webhook_url_env'])
    if not webhook_url:
        print(f"é”™è¯¯: ç¯å¢ƒå˜é‡ {config['discord']['webhook_url_env']} æœªè®¾ç½®ã€‚")
        return

    # æ•°æ®æ ¡éªŒï¼Œç¡®ä¿ Link æ˜¯ç»å¯¹è·¯å¾„
    if not item.get('link') or not item['link'].startswith('http'):
        print(f"è·³è¿‡æ¨é€: æ— æ•ˆæˆ–ç¼ºå¤±çš„é“¾æ¥ '{item.get('link')}'")
        return

    # éšæœºé¢œè‰²
    random_color = random.randint(0, 0xFFFFFF)

    embed = {
        "title": item['title'] if item.get('title') else "æ— æ ‡é¢˜",
        "url": item['link'],
        "color": random_color,
        "image": {"url": item['image']} if item.get('image') and item['image'].startswith('http') else None,
        "footer": {"text": f"æ¥æº: {item['site_name']} â€¢ æ¨é€æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"}
    }

    payload = {
        "username": config['discord']['username'],
        "avatar_url": config['discord']['avatar_url'],
        "embeds": [embed]
    }

    response = requests.post(webhook_url, json=payload)
    if response.status_code == 204:
        print(f"æ¨é€æˆåŠŸ: {item['title']}")
    else:
        print(f"æ¨é€å¤±è´¥: {response.status_code}, {response.text}, Payload: {json.dumps(payload)}")

# å‘é€å¯åŠ¨é€šçŸ¥
def send_startup_notification(config):
    webhook_url = os.environ.get(config['discord']['webhook_url_env'])
    if not webhook_url:
        return

    embed = {
        "title": "ğŸš€ NSFW ç›‘æ§ç³»ç»Ÿå·²å¯åŠ¨",
        "description": f"ç‰ˆæœ¬å· `{__version__}` æ­£åœ¨æ£€æŸ¥æ›´æ–°...",
        "color": random.randint(0, 0xFFFFFF),
        "footer": {"text": f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"}
    }

    payload = {
        "username": config['discord']['username'],
        "avatar_url": config['discord']['avatar_url'],
        "embeds": [embed]
    }
    requests.post(webhook_url, json=payload)

# é‡‡é›†å•ä¸ªç«™ç‚¹
async def scrape_site(page, site_config):
    print(f"Scraping: {site_config['name']} ({site_config['url']})")
    try:
        await page.goto(site_config['url'], timeout=60000, wait_until="domcontentloaded")
    except Exception as e:
        print(f"Navigation error for {site_config['name']}: {e}")
        return []
    
    if site_config.get('is_spa'):
        await asyncio.sleep(3) # ç­‰å¾… SPA åŠ è½½
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await asyncio.sleep(2)

    items_data = []
    items = await page.query_selector_all(site_config['item_list_selector'])
    
    from urllib.parse import urljoin
    
    for item in items:
        try:
            # æå–æ ‡é¢˜
            title = ""
            title_el = await item.query_selector(site_config['title_selector'])
            if title_el:
                # ä¼˜å…ˆå°è¯•å±æ€§è·å– (é’ˆå¯¹ xsnvshen)
                if site_config.get('title_attr'):
                    title = await title_el.get_attribute(site_config['title_attr'])
                if not title:
                    title = await title_el.inner_text()
            
            # æå–é“¾æ¥
            link_el = await item.query_selector(site_config['link_selector'])
            link = await link_el.get_attribute('href') if link_el else ""
            if link:
                link = urljoin(site_config['url'], link)
            
            # æå–å›¾ç‰‡
            img_el = await item.query_selector(site_config['image_selector'])
            image = ""
            if img_el:
                # å°è¯•å¤šç§å›¾ç‰‡å±æ€§ (æ”¯æŒæ‡’åŠ è½½)
                for attr in ['data-original', 'data-src', 'src']:
                    image = await img_el.get_attribute(attr)
                    if image and not image.endswith('.gif') and 'loading' not in image:
                        break
                
                # ç‰¹æ®Šå¤„ç† nshens çš„ background-image
                if not image and site_config['name'] == 'nshens':
                    style = await img_el.get_attribute('style')
                    if style and 'background-image' in style:
                        import re
                        match = re.search(r'url\("(.*?)"\)', style)
                        if match: image = match.group(1)
                
                if image:
                    if image.startswith('//'):
                        image = 'https:' + image
                    image = urljoin(site_config['url'], image)
            
            if link and title:
                items_data.append({
                    'site_name': site_config['name'],
                    'title': title.strip(),
                    'link': link,
                    'image': image
                })
        except Exception:
            pass
            
    return items_data

async def main():
    config = load_config()
    db_path = config['database']['db_path']
    init_db(db_path)

    # å‘é€å¯åŠ¨é€šçŸ¥
    send_startup_notification(config)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        # æ ¸å¿ƒï¼šå¿½ç•¥ HTTPS é”™è¯¯ä»¥é€‚é… xiuren.org
        context = await browser.new_context(ignore_https_errors=True)
        page = await context.new_page()
        
        # ä¼ªè£… User-Agent
        await page.set_extra_http_headers({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        })

        for site_config in config['sites']:
            try:
                results = await scrape_site(page, site_config)
                for item in reversed(results): # é€†åºå¤„ç†ï¼Œä¿è¯æ–°å†…å®¹æœ€åæ¨é€ç‚¹
                    if is_new_link(db_path, item['link']):
                        send_discord_webhook(config, item)
                        save_link(db_path, item['site_name'], item['link'], item['title'])
                    else:
                        # å¦‚æœé‡åˆ°å·²å­˜åœ¨çš„ï¼Œå¯¹äºæŸäº›æŒ‰æ—¶é—´æ’åºçš„ç«™ç‚¹å¯ä»¥æå‰è·³è¿‡
                        pass
            except Exception as e:
                print(f"Error scraping {site_config['name']}: {e}")

        await browser.close()

if __name__ == "__main__":
    asyncio.run(main())
